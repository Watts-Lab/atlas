{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "q_and_a_behavior_df = pd.read_csv('behavior_level.csv')\n",
    "q_and_a_experiment_df = pd.read_csv('experimental_level.csv')\n",
    "\n",
    "master_db_df = pd.read_csv('MasterDB_Full.csv')\n",
    "master_db_df['Paper_Exp_ID_better'] = master_db_df.Paper_Exp_ID.str.extract(r'([A-Z]\\_\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_ids_list = [\n",
    "    \"A_114\",\n",
    "    \"A_116\",\n",
    "    \"A_137\",\n",
    "    \"A_152\",\n",
    "    \"A_168\",\n",
    "    \"A_168\",\n",
    "    \"A_168\",\n",
    "    \"A_181\",\n",
    "    \"A_197\",\n",
    "    \"A_19\",\n",
    "    \"A_228\",\n",
    "    \"A_30\",\n",
    "    \"A_31\",\n",
    "    \"A_39\",\n",
    "    \"A_39\",\n",
    "    \"A_55\",\n",
    "    \"A_62\",\n",
    "    \"A_76\",\n",
    "    \"A_87\",\n",
    "]\n",
    "\n",
    "file_dict = {}\n",
    "\n",
    "folder_path = \"sample_paper\"\n",
    "markdown_pattern = \"*.mmd\"\n",
    "markdown_files = glob.glob(os.path.join(folder_path, markdown_pattern))\n",
    "\n",
    "pattern_id = r'([A-Z]\\_\\d+)'\n",
    "\n",
    "for markdown_file in markdown_files:\n",
    "    match = re.search(pattern_id, markdown_file)\n",
    "    if match:\n",
    "        code = match.group(1)\n",
    "        if code in file_dict:\n",
    "            file_dict[code].append(markdown_file)\n",
    "        else:\n",
    "            file_dict[code] = [markdown_file]\n",
    "    else:\n",
    "        print(\"No match found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_paper/A_19_2022_DoHonestyNudges.mmd']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dict['A_19']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates a vector database from the files for the given article id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def read_files_to_vector(docs: list):\n",
    "    text = \"\"\n",
    "    for d in docs:\n",
    "        text += Path(d).read_text()\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text(text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
    "\n",
    "    vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article class where the vectordb is saved and you can query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "class ArticleQA:\n",
    "    def __init__(self, vector_db, model=\"gpt-4\"):\n",
    "        self.model = model\n",
    "        self.vector_db = vector_db\n",
    "\n",
    "    def query_context(self, question, answer_format):\n",
    "        # Build prompt\n",
    "        template = (\n",
    "            \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
    "        {context}\n",
    "        Question: {question}, Answer format: \"\"\"\n",
    "            + answer_format\n",
    "            + \"\"\",\n",
    "        Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "        llm = ChatOpenAI(model_name=self.model, temperature=0)\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=self.vector_db.as_retriever(\n",
    "                search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "            ),\n",
    "            chain_type=\"stuff\",\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        )\n",
    "\n",
    "        answer_dic = {}\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            result = qa_chain({\"query\": question})\n",
    "            answer_dic[\"gpt_answer\"] = result\n",
    "            answer_dic[\"prompt_tokens\"] = cb.prompt_tokens\n",
    "            answer_dic[\"completion_tokens\"] = cb.completion_tokens\n",
    "            answer_dic[\"total_cost\"] = cb.total_cost\n",
    "\n",
    "        return answer_dic\n",
    "\n",
    "    def query_condition(self, condition, question, answer_format):\n",
    "        # Build prompt\n",
    "        template = (\n",
    "            \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
    "        {context}\n",
    "        You named one of the experimental condition of in this article \"\"\"\n",
    "            + condition\n",
    "            + \"\"\". Can you answer the following question about this condition? \n",
    "        Question: {question}, Answer format: \"\"\"\n",
    "            + answer_format\n",
    "            + \"\"\",\n",
    "        Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "        llm = ChatOpenAI(model_name=self.model, temperature=0)\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=self.vector_db.as_retriever(\n",
    "                search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "            ),\n",
    "            chain_type=\"stuff\",\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        )\n",
    "\n",
    "        answer_dic = {}\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            result = qa_chain({\"query\": question})\n",
    "            answer_dic[\"gpt_answer\"] = result\n",
    "            answer_dic[\"prompt_tokens\"] = cb.prompt_tokens\n",
    "            answer_dic[\"completion_tokens\"] = cb.completion_tokens\n",
    "            answer_dic[\"total_cost\"] = cb.total_cost\n",
    "\n",
    "        return answer_dic\n",
    "\n",
    "    def query_behavior(self, behavior, question, answer_format):\n",
    "        # Build prompt\n",
    "        template = (\n",
    "            \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
    "        {context}\n",
    "        You named one of the behavioral outcomes in this article \"\"\"\n",
    "            + behavior\n",
    "            + \"\"\". Can you answer the following question about this behavior? \\n\n",
    "        Question: {question}, Answer format: \"\"\"\n",
    "            + answer_format\n",
    "            + \"\"\",\n",
    "        Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "        # QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        #     template=template, input_variables=[\"context\", \"question\", \"answer_format\"]\n",
    "        # )\n",
    "\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=self.vector_db.as_retriever(\n",
    "                search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "            ),\n",
    "            chain_type=\"stuff\",\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        )\n",
    "\n",
    "        answer_dic = {}\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            result = qa_chain({\"query\": question})\n",
    "            answer_dic[\"gpt_answer\"] = result\n",
    "            answer_dic[\"prompt_tokens\"] = cb.prompt_tokens\n",
    "            answer_dic[\"completion_tokens\"] = cb.completion_tokens\n",
    "            answer_dic[\"total_cost\"] = cb.total_cost\n",
    "\n",
    "        return answer_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition_behavior_human_answers(master_db, article_id):\n",
    "    condition_keyword_list = [\n",
    "        \"condition_financial\",\n",
    "        \"condition_educational\",\n",
    "        \"condition_forbid\",\n",
    "        \"condition_channel_snailmail\",\n",
    "        \"condition_channel_email\",\n",
    "        \"condition_channel_paperlive\",\n",
    "        \"condition_channel_inperson\",\n",
    "        \"condition_channel_mobileapp\",\n",
    "        \"condition_channel_online\",\n",
    "        \"condition_channel_sms\",\n",
    "        \"condition_channel_phone\",\n",
    "        \"condition_channel_nothing\",\n",
    "    ]\n",
    "\n",
    "    behavior_keywords_list = [\n",
    "        \"behavior_priority\",\n",
    "        \"behavior_focal\",\n",
    "        \"behavior_focal_estimate\",\n",
    "        \"behavior_metric\",\n",
    "        \"behavior_selfreport\",\n",
    "        \"financial_reqt\",\n",
    "        \"travel_reqt\",\n",
    "        \"planning_reqt\",\n",
    "        \"location_reqt\",\n",
    "        \"timing_reqt\",\n",
    "        \"prosociality\",\n",
    "        \"delay\",\n",
    "        \"authority\",\n",
    "    ]\n",
    "\n",
    "    to_process = master_db[master_db[\"Paper_Exp_ID_better\"] == article_id]\n",
    "\n",
    "    first_row_entries_behavior = to_process.groupby(\"behavior_description\").first()[\n",
    "        behavior_keywords_list\n",
    "    ]\n",
    "\n",
    "    first_row_entries_condition = to_process.groupby(\"condition_name\").first()[\n",
    "        condition_keyword_list\n",
    "    ]\n",
    "\n",
    "    return first_row_entries_behavior, first_row_entries_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_gpt_condition_score(master_db, file_dict, paper_exp_id):\n",
    "    A_55 = ArticleQA(read_files_to_vector(file_dict[paper_exp_id]))\n",
    "\n",
    "    behavior_human, condition_human = get_condition_behavior_human_answers(\n",
    "        master_db, paper_exp_id\n",
    "    )\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"Paper_Exp_ID\", \"Condition\", \"Variable\", \"Human Answer\", \"GPT Answer\"]\n",
    "    )\n",
    "\n",
    "    for condition_name in list(condition_human.index):\n",
    "        print(\"Condition:\", condition_name)\n",
    "        for col in list(condition_human.columns):\n",
    "            description_format = q_and_a_experiment_df[\n",
    "                q_and_a_experiment_df[\"Variable name\"] == col\n",
    "            ].values[0]\n",
    "            human_answer = condition_human.loc[condition_name, col]\n",
    "            behavior_question = description_format[2]\n",
    "            behavior_answer_format = description_format[3]\n",
    "\n",
    "            gpt_answer = A_55.query_condition(\n",
    "                condition_name, behavior_question, behavior_answer_format\n",
    "            )\n",
    "            gpt_result = gpt_answer[\"gpt_answer\"][\"result\"]\n",
    "\n",
    "            results_df = results_df.append(\n",
    "                {\n",
    "                    \"Paper_Exp_ID\": paper_exp_id,\n",
    "                    \"Condition\": condition_name,\n",
    "                    \"Variable\": col,\n",
    "                    \"Human Answer\": human_answer,\n",
    "                    \"GPT Answer\": gpt_result,\n",
    "                    \"GPT Cost\": gpt_answer[\"total_cost\"],\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_gpt_behavior_score(master_db, file_dict, paper_exp_id):\n",
    "    A_55 = ArticleQA(read_files_to_vector(file_dict[paper_exp_id]))\n",
    "\n",
    "    behavior_human, condition_human = get_condition_behavior_human_answers(\n",
    "        master_db, paper_exp_id\n",
    "    )\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"Paper_Exp_ID\", \"Behavior\", \"Variable\", \"Human Answer\", \"GPT Answer\"]\n",
    "    )\n",
    "\n",
    "    for behavior_name in list(behavior_human.index):\n",
    "        print(\"Behavior:\", behavior_name)\n",
    "        for col in list(behavior_human.columns):\n",
    "            description_format = q_and_a_behavior_df[\n",
    "                q_and_a_behavior_df[\"Variable name\"] == col\n",
    "            ].values[0]\n",
    "            human_answer = behavior_human.loc[behavior_name, col]\n",
    "            behavior_question = description_format[2]\n",
    "            behavior_answer_format = description_format[3]\n",
    "\n",
    "            gpt_answer = A_55.query_behavior(\n",
    "                behavior_name, behavior_question, behavior_answer_format\n",
    "            )\n",
    "            gpt_result = gpt_answer[\"gpt_answer\"][\"result\"]\n",
    "\n",
    "            results_df = results_df.append(\n",
    "                {\n",
    "                    \"Paper_Exp_ID\": paper_exp_id,\n",
    "                    \"Behavior\": behavior_name,\n",
    "                    \"Variable\": col,\n",
    "                    \"Human Answer\": human_answer,\n",
    "                    \"GPT Answer\": gpt_result,\n",
    "                    \"GPT Cost\": gpt_answer[\"total_cost\"],\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_55 = get_article_gpt_behavior_score(master_db_df, file_dict, \"A_55\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: BAU letter\n",
      "Condition: No letter\n",
      "Condition: Welcome letter\n"
     ]
    }
   ],
   "source": [
    "A_55_c = get_article_gpt_condition_score(master_db_df, file_dict, \"A_55\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_62 = get_article_gpt_behavior_score(master_db_df, file_dict, \"A_62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: control\n",
      "Condition: social norm\n"
     ]
    }
   ],
   "source": [
    "A_62_c = get_article_gpt_condition_score(master_db_df, file_dict, \"A_62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior: Appointment link click\n",
      "Behavior: Days to vaccination\n",
      "Behavior: Vaccination rate within 30 days\n"
     ]
    }
   ],
   "source": [
    "A_87 = get_article_gpt_behavior_score(master_db_df, file_dict, \"A_87\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: Arguments\n",
      "Condition: Control (no reminders)\n",
      "Condition: Control (reminders)\n",
      "Condition: Incentives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: Information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: Social Impact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "A_87_c = get_article_gpt_condition_score(master_db_df, file_dict, \"A_87\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-40iTy1feGwMOIzcYyoUTX6mk on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "A_30 = get_article_gpt_behavior_score(master_db_df, file_dict, \"A_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: control\n",
      "Condition: treatment\n"
     ]
    }
   ],
   "source": [
    "A_30_c = get_article_gpt_condition_score(master_db_df, file_dict, \"A_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper_Exp_ID</th>\n",
       "      <th>Behavior</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Human Answer</th>\n",
       "      <th>GPT Answer</th>\n",
       "      <th>GPT Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_55</td>\n",
       "      <td>change in address</td>\n",
       "      <td>behavior_priority</td>\n",
       "      <td>secondary</td>\n",
       "      <td>none</td>\n",
       "      <td>0.05577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_55</td>\n",
       "      <td>change in address</td>\n",
       "      <td>behavior_focal</td>\n",
       "      <td>not</td>\n",
       "      <td>Vaccination uptake/focal</td>\n",
       "      <td>0.04512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_55</td>\n",
       "      <td>change in address</td>\n",
       "      <td>behavior_focal_estimate</td>\n",
       "      <td>Exact</td>\n",
       "      <td>Estimate</td>\n",
       "      <td>0.04503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_55</td>\n",
       "      <td>change in address</td>\n",
       "      <td>behavior_metric</td>\n",
       "      <td>categorical</td>\n",
       "      <td>The text does not provide information on the v...</td>\n",
       "      <td>0.04767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A_55</td>\n",
       "      <td>change in address</td>\n",
       "      <td>behavior_selfreport</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.04305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>A_30</td>\n",
       "      <td>change in weight, baseline to 24 mo</td>\n",
       "      <td>location_reqt</td>\n",
       "      <td>No</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>0.02091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>A_30</td>\n",
       "      <td>change in weight, baseline to 24 mo</td>\n",
       "      <td>timing_reqt</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.05793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>A_30</td>\n",
       "      <td>change in weight, baseline to 24 mo</td>\n",
       "      <td>prosociality</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.05256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>A_30</td>\n",
       "      <td>change in weight, baseline to 24 mo</td>\n",
       "      <td>delay</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.04479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>A_30</td>\n",
       "      <td>change in weight, baseline to 24 mo</td>\n",
       "      <td>authority</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.02421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>689 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Paper_Exp_ID                             Behavior  \\\n",
       "0           A_55                    change in address   \n",
       "1           A_55                    change in address   \n",
       "2           A_55                    change in address   \n",
       "3           A_55                    change in address   \n",
       "4           A_55                    change in address   \n",
       "..           ...                                  ...   \n",
       "684         A_30  change in weight, baseline to 24 mo   \n",
       "685         A_30  change in weight, baseline to 24 mo   \n",
       "686         A_30  change in weight, baseline to 24 mo   \n",
       "687         A_30  change in weight, baseline to 24 mo   \n",
       "688         A_30  change in weight, baseline to 24 mo   \n",
       "\n",
       "                    Variable Human Answer  \\\n",
       "0          behavior_priority    secondary   \n",
       "1             behavior_focal          not   \n",
       "2    behavior_focal_estimate        Exact   \n",
       "3            behavior_metric  categorical   \n",
       "4        behavior_selfreport            N   \n",
       "..                       ...          ...   \n",
       "684            location_reqt           No   \n",
       "685              timing_reqt           No   \n",
       "686             prosociality           No   \n",
       "687                    delay          Yes   \n",
       "688                authority           No   \n",
       "\n",
       "                                            GPT Answer  GPT Cost  \n",
       "0                                                 none   0.05577  \n",
       "1                             Vaccination uptake/focal   0.04512  \n",
       "2                                             Estimate   0.04503  \n",
       "3    The text does not provide information on the v...   0.04767  \n",
       "4                                                    N   0.04305  \n",
       "..                                                 ...       ...  \n",
       "684                                            Unclear   0.02091  \n",
       "685                                                 No   0.05793  \n",
       "686                                                Yes   0.05256  \n",
       "687                                                Yes   0.04479  \n",
       "688                                                 No   0.02421  \n",
       "\n",
       "[689 rows x 6 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pd.concat([A_55_c, A_62_c, A_87_c, A_30_c], ignore_index=True).to_csv('gpt_condition.csv')\n",
    "\n",
    "pd.concat([A_55, A_62, A_87, A_30], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fec33088580>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_db_df[master_db_df[\"Paper_Exp_ID_better\"] == 'A_30'][behavior_keywords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPT Cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variable</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>condition_channel_email</th>\n",
       "      <td>0.030960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_channel_inperson</th>\n",
       "      <td>0.045113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_channel_mobileapp</th>\n",
       "      <td>0.044105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_channel_nothing</th>\n",
       "      <td>0.049673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_channel_online</th>\n",
       "      <td>0.041615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_channel_paperlive</th>\n",
       "      <td>0.035628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_channel_phone</th>\n",
       "      <td>0.031458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_channel_sms</th>\n",
       "      <td>0.043417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_channel_snailmail</th>\n",
       "      <td>0.033210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_educational</th>\n",
       "      <td>0.042852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_financial</th>\n",
       "      <td>0.061022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_forbid</th>\n",
       "      <td>0.042708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             GPT Cost\n",
       "Variable                             \n",
       "condition_channel_email      0.030960\n",
       "condition_channel_inperson   0.045113\n",
       "condition_channel_mobileapp  0.044105\n",
       "condition_channel_nothing    0.049673\n",
       "condition_channel_online     0.041615\n",
       "condition_channel_paperlive  0.035628\n",
       "condition_channel_phone      0.031458\n",
       "condition_channel_sms        0.043417\n",
       "condition_channel_snailmail  0.033210\n",
       "condition_educational        0.042852\n",
       "condition_financial          0.061022\n",
       "condition_forbid             0.042708"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([A_55_c, A_62_c, A_87_c, A_30_c], ignore_index=True).groupby('Variable').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "master_db_df['condition_financial'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import secrets\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def play(answer, all_words):\n",
    "    possible_words = all_words\n",
    "    word_length = len(answer)\n",
    "\n",
    "    guesses = []\n",
    "    clues = []\n",
    "\n",
    "    for j in range(6):\n",
    "        answer_copy = deepcopy(answer)\n",
    "        guess = secrets.choice(possible_words)\n",
    "\n",
    "        clue = []\n",
    "\n",
    "        for j in range(5):\n",
    "            if guess[j] == answer_copy[j]:\n",
    "                clue.append(\"C\")\n",
    "            else:\n",
    "                if guess[j] in answer_copy:\n",
    "                    clue.append(\"B\")\n",
    "                    answer = answer_copy.replace(\n",
    "                        guess[j], \"0\", 1\n",
    "                    )  # Remove the character from the answer\n",
    "                else:\n",
    "                    clue.append(\"A\")\n",
    "            print(clue, guess, answer_copy, answer)\n",
    "\n",
    "        guesses.append(guess)\n",
    "        clues.append(\"\".join(clue))\n",
    "\n",
    "        if clues[-1] == \"CCCCC\":\n",
    "            break\n",
    "\n",
    "        possible_words = get_possible_words(clues, guesses, answer, possible_words)\n",
    "\n",
    "        print(len(possible_words))\n",
    "        if len(possible_words) == 0:\n",
    "            possible_words = all_words\n",
    "\n",
    "    return {\"guesses\": guesses, \"clues\": clues}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of encoded text: 15988\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# read the file\n",
    "with open('sample_paper/A_19_2022_DoHonestyNudges.mmd', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# encode the text\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "encoded_text = encoding.encode(text)\n",
    "\n",
    "# get the length of the encoded text\n",
    "length = len(encoded_text)\n",
    "\n",
    "print(\"Length of encoded text:\", length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_description_2 = \"\"\"\n",
    "Name the behavioral outcome in a few words. If the authors have a clear, short description of each \\\n",
    "behavior in the paper itself, copy their words here instead of paraphrasing. If their description \\\n",
    "is not clear enough on its own, or is very verbose, please paraphrase here. \\\n",
    "Give a JSON list of each behavioral outcome.\n",
    "\"\"\"\n",
    "\n",
    "A_19 = ArticleQA(read_files_to_vector(['sample_paper/A_19_2022_DoHonestyNudges.mmd']))\n",
    "answer = A_19.query_context(behavior_description_2, 'JSON format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt_answer': {'query': '\\nGive each experiment condition a one or two word name to describe it.  Where possible, use the label the research authors give it. Look at tables/figures to see their naming conventions for the conditions and use those if they exist. Give a JSON list with each experiment condition in it.\\n',\n",
       "  'result': '[\"Business as Usual\", \"Good Practice\", \"Novel Model\"]',\n",
       "  'source_documents': [Document(page_content='1. The sample doesn\\'t capture the digitally excluded, or people not inclined to complete online surveys.\\n2. Just because people are they would do something in an online experiment when playing with \"house money\" doesn\\'t mean they will in real life. We therefore interpret play percentages as an upper bound of real behaviour, and focus primarily on differences between arms.\\n3. Our sample size was chosen to provide adequate statistical power for our main outcomes of interest, and so we recommend interpreting comparisons for subgroups with caution.  \\nAppendix A In the Behavioural task, participants could earn tokens playing a slot game, followed by a final set of questions on sentiment and demographics.', metadata={'Header 2': 'Appendix B Note on interpreting results'}),\n",
       "   Document(page_content='Appendix A The slot adverts we tested were designed iteratively: features identified in a contents analysis were included in mock-ups and user tested with people who gamble.  \\nWe designed five versions of a slot game advert with a different set of features (see Appendix 2), which we compared against the businesses as usual control advert.', metadata={'Header 2': 'Executive Summary'}),\n",
       "   Document(page_content='We have postponed testing the Good Practice arm because of a technical fault, which led to fundamentally different samples between arms.  \\nWhen about 80% of data collection had been completed, we discovered an error with the odds information included in the \"Good Practice\" trial arm.  \\nWe wanted to recruit participants to test the corrected version of the trial arm, but had largely exhausted our usual panel provider\\'s available pool of people who gamble.  \\nWe started recruiting from a new panel, but the participants had different demographics from our existing participants, flagging potential unobservable differences between the groups as well. In particular, they were substantially older and more educated, both of which are strong predictors of gambling behaviour.  \\nAs participants from the new panel would have mostly gone into the Good Practice arm, participants in this arm would likely have differed on both unobservable and observable characteristics from the other arm. This would have violated the fundamental assumption underpinning our ability to identify impact in randomised controlled trials.  \\nWe are considering re-running the experiment for the Good Practice arm compared to the Business as Usual arm, later in 2023.', metadata={'Header 2': 'Appendix 3 - Supporting findings & commentary'}),\n",
       "   Document(page_content='We proposed a novel model for the game advert with a different set of features (see Appendix 2). We proposed a novel model for the game advert with a different set of features (see Appendix 2), which we compared against the businesses as usual control advert.', metadata={'Header 2': '5 Conclusion'}),\n",
       "   Document(page_content='including segmentation analysis and reported behaviour.', metadata={'Header 2': 'Appendix A Additional Methods', 'Header 3': '1.1.1 Exploratory results to support the time-of-step of headline findings'})]},\n",
       " 'prompt_tokens': 622,\n",
       " 'completion_tokens': 15,\n",
       " 'total_cost': 0.01956}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human answer\n",
    "\n",
    "[\n",
    "    \"Claimed amount\",\n",
    "    \"Claim Settlement Diff\",\n",
    "    \"Session Cancellation or not\",\n",
    "    \"Claim rejection or not\",\n",
    "    \"Event description length\",\n",
    "]\n",
    "\n",
    "# GPT 3.5 16k answer\n",
    "[\n",
    "    \"Reduced claimed amount\",\n",
    "    \"Reduced claim-settlement difference\",\n",
    "    \"Increased session cancelation\",\n",
    "    \"Reduced claim rejection\",\n",
    "    \"Increased event description length\",\n",
    "]\n",
    "# GPT 4 answer\n",
    "[\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Claimed Amount\",\n",
    "        \"Description\": \"The amount claimed for settlement by the customer.\",\n",
    "    },\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Claim-Settlement Difference\",\n",
    "        \"Description\": \"The difference between the claimed amount and the final settlement amount issued by the provider.\",\n",
    "    },\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Session Cancelation\",\n",
    "        \"Description\": \"If the customer cancels filling the claim.\",\n",
    "    },\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Claim Rejection\",\n",
    "        \"Description\": \"If the customer’s claims are rejected.\",\n",
    "    },\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Event Description Length\",\n",
    "        \"Description\": \"The number of characters used to describe damages.\",\n",
    "    },\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_description_2 = \"\"\"\n",
    "Name the behavioral outcome in a few words. If the authors have a clear, short description of each \\\n",
    "behavior in the paper itself, copy their words here instead of paraphrasing. If their description \\\n",
    "is not clear enough on its own, or is very verbose, please paraphrase here. \\\n",
    "Give a JSON list of each behavioral outcome.\n",
    "\"\"\"\n",
    "\n",
    "expirment_condition = \"\"\"\n",
    "Give each experiment condition a one or two word name to describe it.  Where possible, use the label the research authors give it. Look at tables/figures to see their naming conventions for the conditions and use those if they exist. Give a JSON list with each experiment condition in it.\n",
    "\"\"\"\n",
    "\n",
    "P_157 = ArticleQA(read_files_to_vector([\"sample_paper/P_157_2023_HowDoSlot_BIT.mmd\"]))\n",
    "answer = P_157.query_context(expirment_condition, \"JSON format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Business as Usual\", \"Good Practice\", \"Novel Model\"]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['gpt_answer']['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 3.5 16k behavioral answer\n",
    "[\n",
    "    \"Most of the slot game advert features tested did not affect behavior on average, nor the perceived chances of winning.\",\n",
    "    \"Listing T&Cs that do not apply more saliently than those that do apply marginally reduced comprehension of the other T&Cs.\",\n",
    "    \"Features that emphasized the game had low risk to potential reward or the 'ease of winning' increased total amount staked.\",\n",
    "    \"Features that emphasized the game 'fun' reduced stakes.\",\n",
    "    \"Features had a differential impact on individuals with high Problem Gambling Severity Index (short-form PGSI) scores and older individuals.\",\n",
    "]\n",
    "# GPT 3.5 16k condition experiment answer\n",
    "[\n",
    "    \"Business as usual\",\n",
    "    \"Low risk to potential reward\",\n",
    "    \"Ease of winning\",\n",
    "    \"Fun-framing\",\n",
    "    \"Good practice\",\n",
    "]\n",
    "\n",
    "# GPT 4 behavioral answer\n",
    "[\n",
    "    \"Comprehension of gambling odds\",\n",
    "    \"Lower-risk gambling guidelines\",\n",
    "    \"DSS gambling legging testing and implementation\",\n",
    "    \"Evaluation of the 'take time to think' safer gambling message\",\n",
    "]\n",
    "# GPT 4 condition experiment answer\n",
    "[\n",
    "    \"Business as Usual\", \n",
    "    \"Good Practice\", \n",
    "    \"Novel Model\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expirment_condition = \"\"\"\n",
    "what are rawls two principles of justice?\n",
    "\"\"\"\n",
    "\n",
    "Rawls = ArticleQA(read_files_to_vector([\"Rawls_Justice_as_Fairness.mmd\"]))\n",
    "answer = Rawls.query_context(expirment_condition, \"free text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"Principle1\": \"Each person must have equal right to the most extensive basic liberty compatible with similar liberty for others.\",\\n\"Principle2\": \"Social and economic inequalities should be arranged so that they are both (a) reasonably expected to be to everyone\\'s advantage, and (b) attached to positions and offices open to all.\"\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['gpt_answer']['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The term \"person\" can be construed in various ways depending on the circumstances. It can refer to human individuals, but also to entities such as nations, provinces, business firms, churches, and teams. The principles of justice apply to all these instances, with a certain logical priority given to human individuals. The term \"person\" is used in an ambiguous manner, sometimes referring to mutually self-interested entities like families or other associations.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rawls.query_context('what are the defenition of persons', \"free text\")['gpt_answer']['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
