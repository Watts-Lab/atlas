{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "q_and_a_behavior_df = pd.read_csv('behavior_level.csv')\n",
    "q_and_a_experiment_df = pd.read_csv('experimental_level.csv')\n",
    "\n",
    "master_db_df = pd.read_csv('MasterDB_Full.csv')\n",
    "master_db_df['Paper_Exp_ID_better'] = master_db_df.Paper_Exp_ID.str.extract(r'([A-Z]\\_\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_ids_list = [\n",
    "    \"A_114\",\n",
    "    \"A_116\",\n",
    "    \"A_137\",\n",
    "    \"A_152\",\n",
    "    \"A_168\",\n",
    "    \"A_168\",\n",
    "    \"A_168\",\n",
    "    \"A_181\",\n",
    "    \"A_197\",\n",
    "    \"A_19\",\n",
    "    \"A_228\",\n",
    "    \"A_30\",\n",
    "    \"A_31\",\n",
    "    \"A_39\",\n",
    "    \"A_39\",\n",
    "    \"A_55\",\n",
    "    \"A_62\",\n",
    "    \"A_76\",\n",
    "    \"A_87\",\n",
    "]\n",
    "\n",
    "file_dict = {}\n",
    "\n",
    "folder_path = \"../server/files/sample_paper\"\n",
    "markdown_pattern = \"*.mmd\"\n",
    "markdown_files = glob.glob(os.path.join(folder_path, markdown_pattern))\n",
    "\n",
    "pattern_id = r'([A-Z]\\_\\d+)'\n",
    "\n",
    "for markdown_file in markdown_files:\n",
    "    match = re.search(pattern_id, markdown_file)\n",
    "    if match:\n",
    "        code = match.group(1)\n",
    "        if code in file_dict:\n",
    "            file_dict[code].append(markdown_file)\n",
    "        else:\n",
    "            file_dict[code] = [markdown_file]\n",
    "    else:\n",
    "        print(\"No match found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../server/files/sample_paper/A_19_2022_DoHonestyNudges.mmd']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dict['A_19']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates a vector database from the files for the given article id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def read_files_to_vector(docs: list):\n",
    "    text = \"\"\n",
    "    for d in docs:\n",
    "        text += Path(d).read_text()\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text(text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=20000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
    "\n",
    "    vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article class where the vectordb is saved and you can query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "class ArticleQA:\n",
    "    def __init__(self, vector_db, model=\"gpt-4-1106-preview\"):\n",
    "        self.model = model\n",
    "        self.vector_db = vector_db\n",
    "\n",
    "    def query_context(self, question, answer_format):\n",
    "        # Build prompt\n",
    "        template = (\n",
    "            \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
    "        {context}\n",
    "        Question: {question}, Answer format: \"\"\"\n",
    "            + answer_format\n",
    "            + \"\"\",\n",
    "        Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "        llm = ChatOpenAI(model_name=self.model, temperature=0)\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=self.vector_db.as_retriever(\n",
    "                search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "            ),\n",
    "            chain_type=\"stuff\",\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        )\n",
    "\n",
    "        answer_dic = {}\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            result = qa_chain({\"query\": question})\n",
    "            answer_dic[\"gpt_answer\"] = result\n",
    "            answer_dic[\"prompt_tokens\"] = cb.prompt_tokens\n",
    "            answer_dic[\"completion_tokens\"] = cb.completion_tokens\n",
    "            answer_dic[\"total_cost\"] = cb.total_cost\n",
    "\n",
    "        return answer_dic\n",
    "\n",
    "    def query_condition(self, condition, question, answer_format):\n",
    "        # Build prompt\n",
    "        template = (\n",
    "            \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
    "        {context}\n",
    "        You named one of the experimental condition of in this article \"\"\"\n",
    "            + condition\n",
    "            + \"\"\". Can you answer the following question about this condition? \n",
    "        Question: {question}, Answer format: \"\"\"\n",
    "            + answer_format\n",
    "            + \"\"\",\n",
    "        Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "        llm = ChatOpenAI(model_name=self.model, temperature=0)\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=self.vector_db.as_retriever(\n",
    "                search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "            ),\n",
    "            chain_type=\"stuff\",\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        )\n",
    "\n",
    "        answer_dic = {}\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            result = qa_chain({\"query\": question})\n",
    "            answer_dic[\"gpt_answer\"] = result\n",
    "            answer_dic[\"prompt_tokens\"] = cb.prompt_tokens\n",
    "            answer_dic[\"completion_tokens\"] = cb.completion_tokens\n",
    "            answer_dic[\"total_cost\"] = cb.total_cost\n",
    "\n",
    "        return answer_dic\n",
    "\n",
    "    def query_behavior(self, behavior, question, answer_format):\n",
    "        # Build prompt\n",
    "        template = (\n",
    "            \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
    "        {context}\n",
    "        You named one of the behavioral outcomes in this article \"\"\"\n",
    "            + behavior\n",
    "            + \"\"\". Can you answer the following question about this behavior? \\n\n",
    "        Question: {question}, Answer format: \"\"\"\n",
    "            + answer_format\n",
    "            + \"\"\",\n",
    "        Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "        # QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        #     template=template, input_variables=[\"context\", \"question\", \"answer_format\"]\n",
    "        # )\n",
    "\n",
    "        llm = ChatOpenAI(model_name=self.model, temperature=0)\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=self.vector_db.as_retriever(\n",
    "                search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "            ),\n",
    "            chain_type=\"stuff\",\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        )\n",
    "\n",
    "        answer_dic = {}\n",
    "\n",
    "        with get_openai_callback() as cb:\n",
    "            result = qa_chain({\"query\": question})\n",
    "            answer_dic[\"gpt_answer\"] = result\n",
    "            answer_dic[\"prompt_tokens\"] = cb.prompt_tokens\n",
    "            answer_dic[\"completion_tokens\"] = cb.completion_tokens\n",
    "            answer_dic[\"total_cost\"] = cb.total_cost\n",
    "\n",
    "        return answer_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition_behavior_human_answers(master_db, article_id):\n",
    "    condition_keyword_list = [\n",
    "        \"condition_financial\",\n",
    "        \"condition_educational\",\n",
    "        \"condition_forbid\",\n",
    "        \"condition_channel_snailmail\",\n",
    "        \"condition_channel_email\",\n",
    "        \"condition_channel_paperlive\",\n",
    "        \"condition_channel_inperson\",\n",
    "        \"condition_channel_mobileapp\",\n",
    "        \"condition_channel_online\",\n",
    "        \"condition_channel_sms\",\n",
    "        \"condition_channel_phone\",\n",
    "        \"condition_channel_nothing\",\n",
    "    ]\n",
    "\n",
    "    behavior_keywords_list = [\n",
    "        \"behavior_priority\",\n",
    "        \"behavior_focal\",\n",
    "        \"behavior_focal_estimate\",\n",
    "        \"behavior_metric\",\n",
    "        \"behavior_selfreport\",\n",
    "        \"financial_reqt\",\n",
    "        \"travel_reqt\",\n",
    "        \"planning_reqt\",\n",
    "        \"location_reqt\",\n",
    "        \"timing_reqt\",\n",
    "        \"prosociality\",\n",
    "        \"delay\",\n",
    "        \"authority\",\n",
    "    ]\n",
    "\n",
    "    to_process = master_db[master_db[\"Paper_Exp_ID_better\"] == article_id]\n",
    "\n",
    "    first_row_entries_behavior = to_process.groupby(\"behavior_description\").first()[\n",
    "        behavior_keywords_list\n",
    "    ]\n",
    "\n",
    "    first_row_entries_condition = to_process.groupby(\"condition_name\").first()[\n",
    "        condition_keyword_list\n",
    "    ]\n",
    "\n",
    "    return first_row_entries_behavior, first_row_entries_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_gpt_condition_score(master_db, file_dict, paper_exp_id):\n",
    "    article = ArticleQA(read_files_to_vector(file_dict[paper_exp_id]))\n",
    "\n",
    "    behavior_human, condition_human = get_condition_behavior_human_answers(\n",
    "        master_db, paper_exp_id\n",
    "    )\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"Paper_Exp_ID\", \"Condition\", \"Variable\", \"Human Answer\", \"GPT Answer\"]\n",
    "    )\n",
    "\n",
    "    for condition_name in list(condition_human.index):\n",
    "        print(\"Condition:\", condition_name)\n",
    "        for col in list(condition_human.columns):\n",
    "            description_format = q_and_a_experiment_df[\n",
    "                q_and_a_experiment_df[\"Variable name\"] == col\n",
    "            ].values[0]\n",
    "            human_answer = condition_human.loc[condition_name, col]\n",
    "            behavior_question = description_format[2]\n",
    "            behavior_answer_format = description_format[3]\n",
    "\n",
    "            gpt_answer = article.query_condition(\n",
    "                condition_name, behavior_question, behavior_answer_format\n",
    "            )\n",
    "            gpt_result = gpt_answer[\"gpt_answer\"][\"result\"]\n",
    "\n",
    "            results_df = results_df.append(\n",
    "                {\n",
    "                    \"Paper_Exp_ID\": paper_exp_id,\n",
    "                    \"Condition\": condition_name,\n",
    "                    \"Variable\": col,\n",
    "                    \"Human Answer\": human_answer,\n",
    "                    \"GPT Answer\": gpt_result,\n",
    "                    \"GPT Cost\": gpt_answer[\"total_cost\"],\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_gpt_behavior_score(master_db, file_dict, paper_exp_id):\n",
    "    article = ArticleQA(read_files_to_vector(file_dict[paper_exp_id]))\n",
    "\n",
    "    behavior_human, condition_human = get_condition_behavior_human_answers(\n",
    "        master_db, paper_exp_id\n",
    "    )\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"Paper_Exp_ID\", \"Behavior\", \"Variable\", \"Human Answer\", \"GPT Answer\"]\n",
    "    )\n",
    "\n",
    "    for behavior_name in list(behavior_human.index):\n",
    "        print(\"Behavior:\", behavior_name)\n",
    "        for col in list(behavior_human.columns):\n",
    "            description_format = q_and_a_behavior_df[\n",
    "                q_and_a_behavior_df[\"Variable name\"] == col\n",
    "            ].values[0]\n",
    "            human_answer = behavior_human.loc[behavior_name, col]\n",
    "            behavior_question = description_format[2]\n",
    "            behavior_answer_format = description_format[3]\n",
    "\n",
    "            gpt_answer = article.query_behavior(\n",
    "                behavior_name, behavior_question, behavior_answer_format\n",
    "            )\n",
    "            gpt_result = gpt_answer[\"gpt_answer\"][\"result\"]\n",
    "\n",
    "            results_df = results_df.append(\n",
    "                {\n",
    "                    \"Paper_Exp_ID\": paper_exp_id,\n",
    "                    \"Behavior\": behavior_name,\n",
    "                    \"Variable\": col,\n",
    "                    \"Human Answer\": human_answer,\n",
    "                    \"GPT Answer\": gpt_result,\n",
    "                    \"GPT Cost\": gpt_answer[\"total_cost\"],\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior: change in address\n",
      "Behavior: lodgment\n",
      "Behavior: myGov registration\n",
      "Behavior: number of days to lodgement\n",
      "Behavior: number of inbound calls\n",
      "Behavior: use of MyTax\n"
     ]
    }
   ],
   "source": [
    "A_55 = get_article_gpt_behavior_score(master_db_df, file_dict, \"A_55\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: BAU letter\n",
      "Condition: No letter\n",
      "Condition: Welcome letter\n"
     ]
    }
   ],
   "source": [
    "A_55_c = get_article_gpt_condition_score(master_db_df, file_dict, \"A_55\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior: Amount automatically saved at t<U+202F>=<U+202F>13 weeks\n",
      "Behavior: Amount automatically saved at t<U+202F>=<U+202F>38 weeks\n",
      "Behavior: Amount automatically saved at t<U+202F>=<U+202F>4 weeks\n",
      "Behavior: Automatic transaction at t<U+202F>=<U+202F>13 weeks\n",
      "Behavior: Automatic transaction at t<U+202F>=<U+202F>38 weeks\n",
      "Behavior: Automatic transaction at t<U+202F>=<U+202F>4 weeks\n",
      "Behavior: Total buffer savings at t<U+202F>=<U+202F>13 weeks\n",
      "Behavior: Total buffer savings at t<U+202F>=<U+202F>38 weeks\n",
      "Behavior: Total buffer savings at t<U+202F>=<U+202F>4 weeks\n",
      "Behavior: click rate - email opt out\n",
      "Behavior: click rate - personalized website\n"
     ]
    }
   ],
   "source": [
    "A_62 = get_article_gpt_behavior_score(master_db_df, file_dict, \"A_62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: control\n",
      "Condition: social norm\n"
     ]
    }
   ],
   "source": [
    "A_62_c = get_article_gpt_condition_score(master_db_df, file_dict, \"A_62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior: Appointment link click\n",
      "Behavior: Days to vaccination\n",
      "Behavior: Vaccination rate within 30 days\n"
     ]
    }
   ],
   "source": [
    "A_87 = get_article_gpt_behavior_score(master_db_df, file_dict, \"A_87\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: Arguments\n",
      "Condition: Control (no reminders)\n",
      "Condition: Control (reminders)\n",
      "Condition: Incentives\n",
      "Condition: Information\n",
      "Condition: Social Impact\n"
     ]
    }
   ],
   "source": [
    "A_87_c = get_article_gpt_condition_score(master_db_df, file_dict, \"A_87\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior: change in BMI, baseline to 12 mo\n",
      "Behavior: change in BMI, baseline to 24 mo\n",
      "Behavior: change in HDL cholesterol, baseline to 12 mo\n",
      "Behavior: change in HDL cholesterol, baseline to 24 mo\n",
      "Behavior: change in LDL cholesterol, baseline to 12 mo\n",
      "Behavior: change in LDL cholesterol, baseline to 24 mo\n",
      "Behavior: change in avg % of green labeled items purchased, baseline to 12 mo\n",
      "Behavior: change in avg % of green labeled items purchased, baseline to 24 mo\n",
      "Behavior: change in avg % of red labeled items purchased, baseline to 12 mo\n",
      "Behavior: change in avg % of red labeled items purchased, baseline to 24 mo\n",
      "Behavior: change in avg % of yellow labeled items purchased, baseline to 12 mo\n",
      "Behavior: change in avg % of yellow labeled items purchased, baseline to 24 mo\n",
      "Behavior: change in avg A1c, baseline to 12 mo\n",
      "Behavior: change in avg A1c, baseline to 24 mo\n",
      "Behavior: change in calories purchased per day, baseline to 12 mo\n",
      "Behavior: change in calories purchased per day, baseline to 24 mo\n",
      "Behavior: change in diastolic BP, baseline to 12 mo\n",
      "Behavior: change in diastolic BP, baseline to 24 mo\n",
      "Behavior: change in diet quality (Healthy Eating Index 2015), baseline to 12 mo\n",
      "Behavior: change in diet quality (Healthy Eating Index 2015), baseline to 24 mo\n",
      "Behavior: change in diet quality (Healthy Eating Index 2015), baseline to 6 mo\n",
      "Behavior: change in healthy purchasing score (avg %), baseline to 12 mo\n",
      "Behavior: change in healthy purchasing score (avg %), baseline to 24 mo\n",
      "Behavior: change in systolic BP, baseline to 12 mo\n",
      "Behavior: change in systolic BP, baseline to 24 mo\n",
      "Behavior: change in total cholesterol, baseline to 12 mo\n",
      "Behavior: change in total cholesterol, baseline to 24 mo\n",
      "Behavior: change in triglycerides, baseline to 12 mo\n",
      "Behavior: change in triglycerides, baseline to 24 mo\n",
      "Behavior: change in waist circumference, baseline to 12 mo\n",
      "Behavior: change in waist circumference, baseline to 24 mo\n",
      "Behavior: change in weight, baseline to 12 mo\n",
      "Behavior: change in weight, baseline to 24 mo\n"
     ]
    }
   ],
   "source": [
    "A_30 = get_article_gpt_behavior_score(master_db_df, file_dict, \"A_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition: control\n",
      "Condition: treatment\n"
     ]
    }
   ],
   "source": [
    "A_30_c = get_article_gpt_condition_score(master_db_df, file_dict, \"A_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_condition = pd.concat([A_55_c, A_62_c, A_87_c, A_30_c], ignore_index=True)\n",
    "gpt_4_turbo_condition[\"Model\"] = \"gpt-4-1106-preview\"\n",
    "\n",
    "gpt_4_turbo_behavior = pd.concat([A_55, A_62, A_87, A_30], ignore_index=True)\n",
    "gpt_4_turbo_behavior[\"Model\"] = \"gpt-4-1106-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_behavior = pd.read_csv(\"gpt_beavhior.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "gpt_4_behavior[\"Model\"] = \"gpt-4\"\n",
    "\n",
    "gpt_4_condition = pd.read_csv(\"gpt_condition.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "gpt_4_condition[\"Model\"] = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([gpt_4_behavior, gpt_4_turbo_behavior], ignore_index=True).to_csv(\n",
    "    \"gpt_behavior.csv\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([gpt_4_condition, gpt_4_turbo_condition], ignore_index=True).to_csv(\n",
    "    \"gpt_condition.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper_Exp_ID</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Human Answer</th>\n",
       "      <th>GPT Answer</th>\n",
       "      <th>GPT Cost</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_55</td>\n",
       "      <td>BAU letter</td>\n",
       "      <td>condition_financial</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.06849</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_55</td>\n",
       "      <td>BAU letter</td>\n",
       "      <td>condition_educational</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.04371</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_55</td>\n",
       "      <td>BAU letter</td>\n",
       "      <td>condition_forbid</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.04713</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_55</td>\n",
       "      <td>BAU letter</td>\n",
       "      <td>condition_channel_snailmail</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>0.04023</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A_55</td>\n",
       "      <td>BAU letter</td>\n",
       "      <td>condition_channel_email</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.03222</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>A_30</td>\n",
       "      <td>treatment</td>\n",
       "      <td>condition_channel_mobileapp</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.04801</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>A_30</td>\n",
       "      <td>treatment</td>\n",
       "      <td>condition_channel_online</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>A_30</td>\n",
       "      <td>treatment</td>\n",
       "      <td>condition_channel_sms</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.03240</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>A_30</td>\n",
       "      <td>treatment</td>\n",
       "      <td>condition_channel_phone</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.03423</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>A_30</td>\n",
       "      <td>treatment</td>\n",
       "      <td>condition_channel_nothing</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.05251</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Paper_Exp_ID   Condition                     Variable Human Answer  \\\n",
       "0           A_55  BAU letter          condition_financial            N   \n",
       "1           A_55  BAU letter        condition_educational            N   \n",
       "2           A_55  BAU letter             condition_forbid            N   \n",
       "3           A_55  BAU letter  condition_channel_snailmail            Y   \n",
       "4           A_55  BAU letter      condition_channel_email            N   \n",
       "..           ...         ...                          ...          ...   \n",
       "307         A_30   treatment  condition_channel_mobileapp            N   \n",
       "308         A_30   treatment     condition_channel_online            N   \n",
       "309         A_30   treatment        condition_channel_sms            N   \n",
       "310         A_30   treatment      condition_channel_phone            N   \n",
       "311         A_30   treatment    condition_channel_nothing            N   \n",
       "\n",
       "    GPT Answer  GPT Cost               Model  \n",
       "0            N   0.06849               gpt-4  \n",
       "1            N   0.04371               gpt-4  \n",
       "2            N   0.04713               gpt-4  \n",
       "3            N   0.04023               gpt-4  \n",
       "4            Y   0.03222               gpt-4  \n",
       "..         ...       ...                 ...  \n",
       "307          N   0.04801  gpt-4-1106-preview  \n",
       "308          Y   0.04870  gpt-4-1106-preview  \n",
       "309          N   0.03240  gpt-4-1106-preview  \n",
       "310          N   0.03423  gpt-4-1106-preview  \n",
       "311          N   0.05251  gpt-4-1106-preview  \n",
       "\n",
       "[312 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('gpt_condition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_description_2 = \"\"\"\n",
    "Name the behavioral outcome in a few words. If the authors have a clear, short description of each \\\n",
    "behavior in the paper itself, copy their words here instead of paraphrasing. If their description \\\n",
    "is not clear enough on its own, or is very verbose, please paraphrase here. \\\n",
    "Give a JSON list of each behavioral outcome.\n",
    "\"\"\"\n",
    "\n",
    "A_19 = ArticleQA(read_files_to_vector(['sample_paper/A_19_2022_DoHonestyNudges.mmd']))\n",
    "answer = A_19.query_context(behavior_description_2, 'JSON format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt_answer': {'query': '\\nGive each experiment condition a one or two word name to describe it.  Where possible, use the label the research authors give it. Look at tables/figures to see their naming conventions for the conditions and use those if they exist. Give a JSON list with each experiment condition in it.\\n',\n",
       "  'result': '[\"Business as Usual\", \"Good Practice\", \"Novel Model\"]',\n",
       "  'source_documents': [Document(page_content='1. The sample doesn\\'t capture the digitally excluded, or people not inclined to complete online surveys.\\n2. Just because people are they would do something in an online experiment when playing with \"house money\" doesn\\'t mean they will in real life. We therefore interpret play percentages as an upper bound of real behaviour, and focus primarily on differences between arms.\\n3. Our sample size was chosen to provide adequate statistical power for our main outcomes of interest, and so we recommend interpreting comparisons for subgroups with caution.  \\nAppendix A In the Behavioural task, participants could earn tokens playing a slot game, followed by a final set of questions on sentiment and demographics.', metadata={'Header 2': 'Appendix B Note on interpreting results'}),\n",
       "   Document(page_content='Appendix A The slot adverts we tested were designed iteratively: features identified in a contents analysis were included in mock-ups and user tested with people who gamble.  \\nWe designed five versions of a slot game advert with a different set of features (see Appendix 2), which we compared against the businesses as usual control advert.', metadata={'Header 2': 'Executive Summary'}),\n",
       "   Document(page_content='We have postponed testing the Good Practice arm because of a technical fault, which led to fundamentally different samples between arms.  \\nWhen about 80% of data collection had been completed, we discovered an error with the odds information included in the \"Good Practice\" trial arm.  \\nWe wanted to recruit participants to test the corrected version of the trial arm, but had largely exhausted our usual panel provider\\'s available pool of people who gamble.  \\nWe started recruiting from a new panel, but the participants had different demographics from our existing participants, flagging potential unobservable differences between the groups as well. In particular, they were substantially older and more educated, both of which are strong predictors of gambling behaviour.  \\nAs participants from the new panel would have mostly gone into the Good Practice arm, participants in this arm would likely have differed on both unobservable and observable characteristics from the other arm. This would have violated the fundamental assumption underpinning our ability to identify impact in randomised controlled trials.  \\nWe are considering re-running the experiment for the Good Practice arm compared to the Business as Usual arm, later in 2023.', metadata={'Header 2': 'Appendix 3 - Supporting findings & commentary'}),\n",
       "   Document(page_content='We proposed a novel model for the game advert with a different set of features (see Appendix 2). We proposed a novel model for the game advert with a different set of features (see Appendix 2), which we compared against the businesses as usual control advert.', metadata={'Header 2': '5 Conclusion'}),\n",
       "   Document(page_content='including segmentation analysis and reported behaviour.', metadata={'Header 2': 'Appendix A Additional Methods', 'Header 3': '1.1.1 Exploratory results to support the time-of-step of headline findings'})]},\n",
       " 'prompt_tokens': 622,\n",
       " 'completion_tokens': 15,\n",
       " 'total_cost': 0.01956}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human answer\n",
    "\n",
    "[\n",
    "    \"Claimed amount\",\n",
    "    \"Claim Settlement Diff\",\n",
    "    \"Session Cancellation or not\",\n",
    "    \"Claim rejection or not\",\n",
    "    \"Event description length\",\n",
    "]\n",
    "\n",
    "# GPT 3.5 16k answer\n",
    "[\n",
    "    \"Reduced claimed amount\",\n",
    "    \"Reduced claim-settlement difference\",\n",
    "    \"Increased session cancelation\",\n",
    "    \"Reduced claim rejection\",\n",
    "    \"Increased event description length\",\n",
    "]\n",
    "# GPT 4 answer\n",
    "[\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Claimed Amount\",\n",
    "        \"Description\": \"The amount claimed for settlement by the customer.\",\n",
    "    },\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Claim-Settlement Difference\",\n",
    "        \"Description\": \"The difference between the claimed amount and the final settlement amount issued by the provider.\",\n",
    "    },\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Session Cancelation\",\n",
    "        \"Description\": \"If the customer cancels filling the claim.\",\n",
    "    },\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Claim Rejection\",\n",
    "        \"Description\": \"If the customer’s claims are rejected.\",\n",
    "    },\n",
    "    {\n",
    "        \"Behavioral Outcome\": \"Event Description Length\",\n",
    "        \"Description\": \"The number of characters used to describe damages.\",\n",
    "    },\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_description_2 = \"\"\"\n",
    "Name the behavioral outcome in a few words. If the authors have a clear, short description of each \\\n",
    "behavior in the paper itself, copy their words here instead of paraphrasing. If their description \\\n",
    "is not clear enough on its own, or is very verbose, please paraphrase here. \\\n",
    "Give a JSON list of each behavioral outcome.\n",
    "\"\"\"\n",
    "\n",
    "expirment_condition = \"\"\"\n",
    "Give each experiment condition a one or two word name to describe it.  Where possible, use the label the research authors give it. Look at tables/figures to see their naming conventions for the conditions and use those if they exist. Give a JSON list with each experiment condition in it.\n",
    "\"\"\"\n",
    "\n",
    "P_157 = ArticleQA(read_files_to_vector([\"sample_paper/P_157_2023_HowDoSlot_BIT.mmd\"]))\n",
    "answer = P_157.query_context(expirment_condition, \"JSON format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Business as Usual\", \"Good Practice\", \"Novel Model\"]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['gpt_answer']['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 3.5 16k behavioral answer\n",
    "[\n",
    "    \"Most of the slot game advert features tested did not affect behavior on average, nor the perceived chances of winning.\",\n",
    "    \"Listing T&Cs that do not apply more saliently than those that do apply marginally reduced comprehension of the other T&Cs.\",\n",
    "    \"Features that emphasized the game had low risk to potential reward or the 'ease of winning' increased total amount staked.\",\n",
    "    \"Features that emphasized the game 'fun' reduced stakes.\",\n",
    "    \"Features had a differential impact on individuals with high Problem Gambling Severity Index (short-form PGSI) scores and older individuals.\",\n",
    "]\n",
    "# GPT 3.5 16k condition experiment answer\n",
    "[\n",
    "    \"Business as usual\",\n",
    "    \"Low risk to potential reward\",\n",
    "    \"Ease of winning\",\n",
    "    \"Fun-framing\",\n",
    "    \"Good practice\",\n",
    "]\n",
    "\n",
    "# GPT 4 behavioral answer\n",
    "[\n",
    "    \"Comprehension of gambling odds\",\n",
    "    \"Lower-risk gambling guidelines\",\n",
    "    \"DSS gambling legging testing and implementation\",\n",
    "    \"Evaluation of the 'take time to think' safer gambling message\",\n",
    "]\n",
    "# GPT 4 condition experiment answer\n",
    "[\n",
    "    \"Business as Usual\", \n",
    "    \"Good Practice\", \n",
    "    \"Novel Model\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
