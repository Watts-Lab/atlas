{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dotenv\n",
        "import os\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question and answer prompt generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YClKlm3oeu5W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "q_and_a_df = pd.read_csv(\"A_111_1_vF for Mark - Experiment Level.csv\")\n",
        "\n",
        "q_and_a_df[\"How to code\"] = q_and_a_df[\"How to code\"].replace(\n",
        "    {\"Y/N\": \"Y/N (yes or no)\", \"Y/N/Unclear\": \"Y/N/Unclear (Yes, No or Unclear)\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for label, question, answer_desc in q_and_a_df[[\"Variable\", \"Description\", \"How to code\"]].head(5).values:\n",
        "    print(f\"Label: {label}, Question: {question}, Answer: {answer_desc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------\n",
        "## Markdown split and QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_and_a_behavior_df = pd.read_csv('behavior_level.csv')\n",
        "\n",
        "q_and_a_experiment_df = pd.read_csv('experimental_level.csv')\n",
        "\n",
        "q_and_a_behavior_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for vars in q_and_a_experiment_df['Variable name'].unique():\n",
        "    if vars in article_level:\n",
        "        print(f\"{vars} - Article level\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "masterdb_df['Paper_Exp_ID_better'] = masterdb_df.Paper_Exp_ID.str.extract(r'([A-Z]\\_\\d+)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[v for v in article_level if v not in relevant_columns]\n",
        "article_level\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "article_level = []\n",
        "experiment_level = []\n",
        "\n",
        "for colomn in masterdb_df.columns:\n",
        "    if (\n",
        "        masterdb_df.groupby(\"Paper_Exp_ID_better\")[colomn]\n",
        "        .nunique()\n",
        "        .reset_index(name=\"unique_count\")\n",
        "        .sort_values([\"unique_count\"], ascending=False)\n",
        "        .max()[\"unique_count\"]\n",
        "        == 1\n",
        "    ):\n",
        "        article_level.append(colomn)\n",
        "    else:\n",
        "        experiment_level.append(colomn)\n",
        "\n",
        "print(f\"{len(article_level)} - Article level: {article_level} \")\n",
        "print(f\"{len(experiment_level)} - Experiment level: {experiment_level}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for at in article_level:\n",
        "    print(at)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "relevant_columns = []\n",
        "\n",
        "for vars in q_and_a_df.Variable.unique():\n",
        "    if vars in article_level:\n",
        "        print(f\"{vars} - Article level\")\n",
        "        relevant_columns.append(vars)\n",
        "\n",
        "len(relevant_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "masterdb_df = pd.read_csv('MasterDB_Full.csv')\n",
        "\n",
        "pattern = r'([A-Z]\\_\\d+)'\n",
        "\n",
        "available_ids = []\n",
        "\n",
        "for id in list(masterdb_df.groupby('Paper_Exp_ID').count().index):\n",
        "    match = re.search(pattern, id)\n",
        "    if match:\n",
        "        extracted = match.group(1)\n",
        "        available_ids.append(extracted)\n",
        "    else:\n",
        "        print(\"No match found.\")\n",
        "\n",
        "len(available_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "available_markdown_files = []\n",
        "\n",
        "folder_path = \"markdown_articles\"\n",
        "markdown_pattern = \"*.mmd\"\n",
        "markdown_files = glob.glob(os.path.join(folder_path, markdown_pattern))\n",
        "\n",
        "pattern = r'[^/]+/([A-Z0-9_]+_[0-9]+[a-zA-Z]*_[0-9]+)'\n",
        "pattern_id = r'([A-Z]\\_\\d+)'\n",
        "\n",
        "for markdown_file in markdown_files:\n",
        "    # print(\"Processing:\", markdown_file)\n",
        "\n",
        "    # with open(markdown_file, 'r', encoding='utf-8') as file:\n",
        "    #     markdown_content = file.read()\n",
        "    \n",
        "    # file.close()\n",
        "    match = re.search(pattern_id, markdown_file)\n",
        "    if match:\n",
        "        code = match.group(1)\n",
        "        if code in available_ids:\n",
        "            # print(\"Found:\", code)\n",
        "            available_markdown_files.append(markdown_file)\n",
        "    else:\n",
        "        print(\"No match found.\")\n",
        "\n",
        "\n",
        "file_name = \"markdown_articles/A_1_2022_ADigitalApproach.mmd\"\n",
        "\n",
        "\n",
        "len(available_markdown_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_label(file_name):\n",
        "    # pattern = r'[^/]+/([A-Z0-9_]+_[0-9]+[a-zA-Z]*_[0-9]+)'\n",
        "    pattern = r'([A-Z]\\_\\d+)'\n",
        "    match = re.search(pattern, file_name)\n",
        "    if match:\n",
        "        code = match.group(1)\n",
        "        return code\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "masterdb_df[masterdb_df['Paper_Exp_ID'].str.contains('A_114')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "def create_vector_db(file_name):\n",
        "    loader = TextLoader(file_name)\n",
        "    markdown_doc = loader.load()\n",
        "\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "        (\"####\", \"Header 4\"),\n",
        "    ]\n",
        "\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    md_header_splits = markdown_splitter.split_text(markdown_doc[0].page_content)\n",
        "\n",
        "    # Define the Text Splitter\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
        "\n",
        "    # Create a split of the document using the text splitter\n",
        "    splits = text_splitter.split_documents(md_header_splits)\n",
        "\n",
        "\n",
        "    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
        "\n",
        "    persist_directory = \"./docs/chroma/\"\n",
        "\n",
        "    # Create the vector store\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=splits, embedding=embedding#, persist_directory=persist_directory\n",
        "    )\n",
        "\n",
        "    return vectordb\n",
        "\n",
        "    # print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def get_answer_format(vectordb, question, answer):\n",
        "    \n",
        "    # Build prompt\n",
        "    template = \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
        "    {context}\n",
        "    Question: {question}, Answer format: \"\"\"+answer+\"\"\",\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "    # QA_CHAIN_PROMPT = PromptTemplate(\n",
        "    #     template=template, input_variables=[\"context\", \"question\", \"answer_format\"]\n",
        "    # )\n",
        "\n",
        "    llm = ChatOpenAI(model_name=\"gpt-4-32k\", temperature=0)\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm,\n",
        "        retriever=vectordb.as_retriever(\n",
        "            search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
        "        ),\n",
        "        chain_type=\"stuff\",\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        "    )\n",
        "\n",
        "    result = qa_chain({\"query\": question})\n",
        "\n",
        "    # return result\n",
        "    return result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "# print(question)\n",
        "# print(answer_desc)\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    vectordb = create_vector_db(\"sample_paper/A_19_2022_DoHonestyNudges.mmd\")\n",
        "    print(get_answer_format(vectordb, 'how is the paper', 'free text'))\n",
        "    print(f\"Prompt Tokens: ${cb.prompt_tokens}\")\n",
        "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "answer_dic = {\"label\": [], \"gpt_answer\": []}\n",
        "\n",
        "for label, question, answer_desc in (\n",
        "    q_and_a_df[[\"Variable\", \"Description\", \"How to code\"]].head(25).values\n",
        "):\n",
        "    print(f\"Label: {label}, Answer: {answer_desc}\")\n",
        "    answer_dic[\"label\"].append(label)\n",
        "    answer_dic[\"gpt_answer\"].append(get_answer_format(vectordb, question, answer_desc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "answer_df = pd.DataFrame(answer_dic)\n",
        "answer_df[\"human_answer\"] = \"\"\n",
        "answer_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_dataframes_others = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for md_file_name in available_markdown_files[:20]:\n",
        "    # get the label id\n",
        "    label_id = get_label(md_file_name)\n",
        "    # seperate the answered section for that id\n",
        "    md_df = masterdb_df[masterdb_df[\"Paper_Exp_ID\"].str.contains(label_id)]\n",
        "\n",
        "    # create a vector db for the article text\n",
        "    file_vector_db = create_vector_db(md_file_name)\n",
        "\n",
        "    answer_dic = {\"label\": [], \"gpt_answer\": [], \"prompt_tokens\": [], \"completion_tokens\": [], \"total_cost\": []}\n",
        "\n",
        "    for label, question, answer_desc in (\n",
        "        # get the relevant rows from the q_and_a_df\n",
        "        q_and_a_df[q_and_a_df['Variable'].isin(relevant_columns)][[\"Variable\", \"Description\", \"How to code\"]].values\n",
        "    ):\n",
        "        print(f\"Label: {label}, Answer: {answer_desc}\")\n",
        "        answer_dic[\"label\"].append(label)\n",
        "        with get_openai_callback() as cb:\n",
        "            chat_result = get_answer_format(file_vector_db, question, answer_desc)\n",
        "            answer_dic[\"gpt_answer\"].append(chat_result)\n",
        "            answer_dic[\"prompt_tokens\"].append(cb.prompt_tokens)\n",
        "            answer_dic[\"completion_tokens\"].append(cb.completion_tokens)\n",
        "            answer_dic[\"total_cost\"].append(cb.total_cost)\n",
        "\n",
        "    answer_df = pd.DataFrame(answer_dic)\n",
        "    answer_df[\"human_answer\"] = \"\"\n",
        "\n",
        "    # take the first row of the md_df and add as human answer\n",
        "    my_dict = md_df.iloc[0].to_dict()\n",
        "\n",
        "    for key, value in my_dict.items():\n",
        "        mask = answer_df[\"label\"] == key\n",
        "        answer_df.loc[mask, \"human_answer\"] = value\n",
        "\n",
        "    answer_df[\"article_id\"] = label_id\n",
        "\n",
        "    all_dataframes.append(answer_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_dataframes_others = []\n",
        "\n",
        "for md_file_name in available_markdown_files[:20]:\n",
        "    # get the label id\n",
        "    label_id = get_label(md_file_name)\n",
        "    # seperate the answered section for that id\n",
        "    md_df = masterdb_df[masterdb_df[\"Paper_Exp_ID\"].str.contains(label_id)]\n",
        "\n",
        "    # create a vector db for the article text\n",
        "    file_vector_db = create_vector_db(md_file_name)\n",
        "\n",
        "    answer_dic = {\"label\": [], \"gpt_answer\": [], \"prompt_tokens\": [], \"completion_tokens\": [], \"total_cost\": []}\n",
        "\n",
        "    for label, question, answer_desc in (\n",
        "        # get the relevant rows from the q_and_a_df\n",
        "        q_and_a_df[[\"Variable\", \"Description\", \"How to code\"]].head(25).values\n",
        "    ):\n",
        "        print(f\"Label: {label}, Answer: {answer_desc}\")\n",
        "        answer_dic[\"label\"].append(label)\n",
        "        with get_openai_callback() as cb:\n",
        "            chat_result = get_answer_format(file_vector_db, question, answer_desc)\n",
        "            answer_dic[\"gpt_answer\"].append(chat_result)\n",
        "            answer_dic[\"prompt_tokens\"].append(cb.prompt_tokens)\n",
        "            answer_dic[\"completion_tokens\"].append(cb.completion_tokens)\n",
        "            answer_dic[\"total_cost\"].append(cb.total_cost)\n",
        "\n",
        "    answer_df = pd.DataFrame(answer_dic)\n",
        "    answer_df[\"human_answer\"] = \"\"\n",
        "\n",
        "    # take the first row of the md_df and add as human answer\n",
        "    my_dict = md_df.iloc[0].to_dict()\n",
        "\n",
        "    for key, value in my_dict.items():\n",
        "        mask = answer_df[\"label\"] == key\n",
        "        answer_df.loc[mask, \"human_answer\"] = value\n",
        "\n",
        "    answer_df[\"article_id\"] = label_id\n",
        "\n",
        "    all_dataframes_others.append(answer_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_df = pd.concat(all_dataframes_others, ignore_index=True)\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "result_df.gpt_answer.replace({\"Yes\": \"Y\", \"No\": \"N\", \"\": \"NA\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_df.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_df.fillna(\"NA\", inplace=True)\n",
        "result_df['comparison'] = result_df.apply(lambda x: x['gpt_answer'] == x['human_answer'], axis=1)\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_df.iloc[2].gpt_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_df.groupby('article_id').sum()['total_cost']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_theme(style=\"white\")\n",
        "\n",
        "\n",
        "colors = [\"red\", \"green\"]\n",
        "\n",
        "\n",
        "plot = sns.relplot(\n",
        "    x=\"article_id\",\n",
        "    y=\"label\",\n",
        "    hue=\"comparison\",\n",
        "    style=\"comparison\",\n",
        "    markers=\"s\",\n",
        "    s=3000,\n",
        "    height=15,\n",
        "    data=result_df,\n",
        "    palette=colors,  # Set the color palette\n",
        ")\n",
        "\n",
        "\n",
        "plot._legend.remove()\n",
        "\n",
        "\n",
        "custom_legend_labels = [\"False\", \"True\"]\n",
        "for color, label in zip(colors, custom_legend_labels):\n",
        "    plt.scatter([], [], c=color, marker=\"s\", label=label)  # Use 's' for squares\n",
        "\n",
        "\n",
        "plot.ax.set_aspect(\"equal\")\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def highlight_truevalues(comp):\n",
        "    # is_max = pd.Series(data=False, index=s.index)\n",
        "    # is_max[column] = s.loc[column] == True\n",
        "    # return ['background-color: green' if is_max.any() else 'background-color: red' for v in is_max]\n",
        "    if comp:\n",
        "        return 'background-color: green'\n",
        "    else:\n",
        "        return 'background-color: red'\n",
        "\n",
        "result_df.style.applymap(highlight_truevalues, subset=['comparison'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_df.groupby('article_id')['comparison'].value_counts().unstack(fill_value=0).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnuEroYHZbxJ"
      },
      "source": [
        "-----------------------------------------------------------------------------\n",
        "\n",
        "## Exploring an behavioral and experimental level questions\n",
        "\n",
        "looping through parts of a conceptual experiment structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## setup question retrieval\n",
        "\n",
        "condition_df = pd.read_csv('experimental_level.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredXMLLoader\n",
        "\n",
        "loader = UnstructuredXMLLoader(\n",
        "    \"A_140a_2020_PromotingVoterRegistration.xml\",\n",
        "    mode=\"elements\",\n",
        "    strategy=\"fast\",\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "# md_header_splits = markdown_splitter.split_text(markdown_doc[0].page_content)\n",
        "\n",
        "# # Define the Text Splitter\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
        "\n",
        "# # Create a split of the document using the text splitter\n",
        "# splits = text_splitter.split_documents(md_header_splits)\n",
        "\n",
        "\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
        "\n",
        "# Create a split of the document using the text splitter\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Create the vector store\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    collection_name=\"A_140a_2020_PromotingVoterRegistration\"\n",
        "    #   persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "\n",
        "vectordb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "def create_vector_db(file_name):\n",
        "    loader = TextLoader(file_name)\n",
        "    markdown_doc = loader.load()\n",
        "\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "        (\"####\", \"Header 4\"),\n",
        "    ]\n",
        "\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    md_header_splits = markdown_splitter.split_text(markdown_doc[0].page_content)\n",
        "\n",
        "    # Define the Text Splitter\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
        "\n",
        "    # Create a split of the document using the text splitter\n",
        "    splits = text_splitter.split_documents(md_header_splits)\n",
        "\n",
        "\n",
        "    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)\n",
        "\n",
        "    persist_directory = \"./docs/chroma/\"\n",
        "\n",
        "    # Create the vector store\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=splits, embedding=embedding#, persist_directory=persist_directory\n",
        "    )\n",
        "\n",
        "    return vectordb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A_140_db = create_vector_db('markdown_articles/A_111a_2021_UsingFreshStarts.mmd')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A_140_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def get_answer_format(vectordb, question, answer):\n",
        "    \n",
        "    # Build prompt\n",
        "    template = \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
        "    {context}\n",
        "    Question: {question}, Answer format: \"\"\"+answer+\"\"\",\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "    llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm,\n",
        "        retriever=vectordb.as_retriever(\n",
        "            search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
        "        ),\n",
        "        chain_type=\"stuff\",\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        "    )\n",
        "\n",
        "    result = qa_chain({\"query\": question})\n",
        "\n",
        "    # return result\n",
        "    return result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "condition_df.iloc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_answer_format(A_140_db, condition_df.iloc[0]['Explanation'], condition_df.iloc[0]['Coding scheme'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "multiline_text = \"\"\"\n",
        "You answered, {answer} regarding the question, {question_1} about the given document.\n",
        "\n",
        "here is a follow up question: {question_2}\n",
        "\"\"\"\n",
        "\n",
        "# Define variables to replace placeholders\n",
        "variables = {\n",
        "    \"answer\": \"Birthday Framing\",\n",
        "    \"question_1\": \"Give each condition a one or two word name to describe it. Author's words: Where possible, use the label the research authors give it. Look at tables/figures to see their naming conventions for the conditions and use those if they exist.?\",\n",
        "    \"question_2\": condition_df.iloc[1]['Explanation']\n",
        "}\n",
        "\n",
        "# Use string formatting to replace placeholders with variables\n",
        "formatted_text = multiline_text.format(**variables)\n",
        "\n",
        "get_answer_format(A_140_db, formatted_text, condition_df.iloc[1]['Coding scheme'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_one = \"The 'Holiday Framing Condition' refers to a method used in the study where the time delay for a certain action (in this case, retirement plan contributions) was framed in reference to a holiday. For example, instead of telling an employee that they would start contributing to their retirement plan in three months, they would be told that they would start contributing after a specific holiday, such as New Year\\'s or the first day of spring. The study included several different holiday framing conditions, including Thanksgiving, Martin Luther King Day, Valentine\\'s Day, New Year\\'s, and the first day of spring. The purpose of this framing was to investigate whether associating the start of contributions with a significant temporal landmark like a holiday would influence the likelihood of employees taking up the delayed savings opportunity.\"\n",
        "\n",
        "sentence_two = \"given the opportunity to sign up to save/increase savings either immediately or on a given future date in the next year -- the future date is described in terms of holidays that are not associated with beginning something new (Thanksgiving, MLK Day, and Valentine's Day) - cohort 1 only\"\n",
        "\n",
        "sentence_similarity_gpt4(sentence_one, sentence_two)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    A_140_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}),\n",
        "    chain_type=\"stuff\",\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = (\n",
        "    \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
        "{context}\n",
        "Question: {question}, Answer format: \"\"\"\n",
        "    +  condition_df.iloc[0]['Coding scheme']\n",
        "    + \"\"\",\n",
        "Answer:\"\"\"\n",
        ")\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = condition_df.iloc[0]['Explanation']\n",
        "result = qa({\"question\": query})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query2 = \"For the condition 2. Holiday Framing Condition \" + condition_df.iloc[1][\"Explanation\"]\n",
        "result = qa({\"question\": query2})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query3 = \"For the condition 'Holiday Framing Condition' \" + condition_df.iloc[2][\"Explanation\"]\n",
        "result = qa({\"question\": query3})\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------------------\n",
        "## Behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "behavior_df = pd.read_csv('behavior_level.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    A_140_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}),\n",
        "    chain_type=\"stuff\",\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "multiline_text = (\n",
        "    \"\"\" Question: \"\"\"\n",
        "    + behavior_df.iloc[0][\"Explanation\"]\n",
        "    + \"\"\", Answer format: \"\"\"\n",
        "    + behavior_df.iloc[0][\"Coding scheme\"]\n",
        ")\n",
        "\n",
        "result = qa({\"question\": multiline_text})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "masterdb_df.groupby('Paper_Exp_ID').nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "masterdb_df.groupby('Paper_Exp_ID').nunique()[list(condition_df['Variable name'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# keywords_list = [\n",
        "#     \"condition_financial\",\n",
        "#     \"condition_educational\",\n",
        "#     \"condition_forbid\",\n",
        "#     \"condition_channel_snailmail\",\n",
        "#     \"condition_channel_email\",\n",
        "#     \"condition_channel_paperlive\",\n",
        "#     \"condition_channel_inperson\",\n",
        "#     \"condition_channel_mobileapp\",\n",
        "#     \"condition_channel_online\",\n",
        "#     \"condition_channel_sms\",\n",
        "#     \"condition_channel_phone\",\n",
        "#     \"condition_channel_nothing\"\n",
        "# ]\n",
        "\n",
        "keywords_list= [\"behavior_priority\",\n",
        "\"behavior_focal\",\n",
        "\"behavior_focal_estimate\",\n",
        "\"behavior_metric\",\n",
        "\"behavior_selfreport\",\n",
        "\"financial_reqt\",\n",
        "\"travel_reqt\",\n",
        "\"planning_reqt\",\n",
        "\"location_reqt\",\n",
        "\"timing_reqt\",\n",
        "\"prosociality\",\n",
        "\"delay\",\n",
        "\"authority\"]\n",
        "\n",
        "masterdb_df.groupby('Paper_Exp_ID').nunique()[keywords_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the count of unique entries in each column from keywords_list for each group\n",
        "unique_counts = masterdb_df.groupby('Paper_Exp_ID')[keywords_list].nunique()\n",
        "\n",
        "# Count the number of columns where unique count is greater than 1 for each group\n",
        "num_columns_with_unique_count_greater_than_1 = (unique_counts > 1).sum(axis=1)\n",
        "\n",
        "# Filter the groups where more than 4 columns have unique count greater than 1\n",
        "result = unique_counts[num_columns_with_unique_count_greater_than_1 > 3]\n",
        "\n",
        "result.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------\n",
        "## Custom prompt per variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "q_and_a_behavior_df = pd.read_csv(\"behavior_level.csv\")\n",
        "\n",
        "q_and_a_experiment_df = pd.read_csv(\"experimental_level.csv\")\n",
        "\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
        "        If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
        "        {context}\n",
        "        Question: {question}, Answer format: {{answer}}\n",
        "        Answer:\"\"\"\n",
        "\n",
        "output_dic_prompt = {}\n",
        "\n",
        "output_dic_prompt[q_and_a_behavior_df.loc[0, \"Variable name\"]] = template.format(\n",
        "    question=q_and_a_behavior_df.loc[0, \"Explanation\"],\n",
        "    context=\"\",\n",
        ")\n",
        "\n",
        "json.dumps(output_dic_prompt)\n",
        "\n",
        "with open(\"behavior_prompt.json\", \"w\") as outfile:\n",
        "    json.dump(output_dic_prompt, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_and_a_behavior_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt engineer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import csv\n",
        "import json\n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "openai.organization = os.getenv(\"OPENAI_ORGANIZATION\")\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "def sentence_similarity_gpt4(question, gpt_answer, human_answer):\n",
        "    systemPromt = \"You are a bot that compares the similarity of an answer between gpt answer and human answer to the same question, you take the\\\n",
        "        context of them into consideration and you will only return the score which is between 0 and 1. You are given the question, the desired answer and the gpt answer.\"\n",
        "    userPrompt = \"Question: {question} \\nGPT answer: {A}\\nHuman answer: {B}\".format(\n",
        "        question=question, A=gpt_answer, B=human_answer\n",
        "    )\n",
        "\n",
        "    functions = [\n",
        "        {\n",
        "            \"name\": \"get_similarity_score\",\n",
        "            \"description\": \"get the similarity between two sentences\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"score\": {\n",
        "                        \"type\": \"number\",\n",
        "                        \"description\": \"the similarity score between two sentences between 0 and 1\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"score\"],\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        # model=\"gpt-4\",\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": systemPromt},\n",
        "            {\"role\": \"user\", \"content\": userPrompt},\n",
        "        ],\n",
        "        temperature=0,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",\n",
        "    )\n",
        "\n",
        "    return completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "\n",
        "class ArticleQA:\n",
        "    def __init__(self, vector_db, model=\"gpt-4\"):\n",
        "        self.model = model\n",
        "        self.vector_db = vector_db\n",
        "\n",
        "    def query_context(self, question, answer_format):\n",
        "        # Build prompt\n",
        "        template = (\n",
        "            \"\"\"Use the following pieces of context to answer the question at the end according to the format of the answer provided. \n",
        "        If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \n",
        "        {context}\n",
        "        Question: {question}, Answer format: \"\"\"\n",
        "            + answer_format\n",
        "            + \"\"\",\n",
        "        Answer:\"\"\"\n",
        "        )\n",
        "\n",
        "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "        llm = ChatOpenAI(model_name=self.model, temperature=0)\n",
        "\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm,\n",
        "            retriever=self.vector_db.as_retriever(\n",
        "                search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
        "            ),\n",
        "            chain_type=\"stuff\",\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        "        )\n",
        "\n",
        "        answer_dic = {}\n",
        "\n",
        "        with get_openai_callback() as cb:\n",
        "            result = qa_chain({\"query\": question})\n",
        "            answer_dic[\"gpt_answer\"] = result\n",
        "            answer_dic[\"prompt_tokens\"] = cb.prompt_tokens\n",
        "            answer_dic[\"completion_tokens\"] = cb.completion_tokens\n",
        "            answer_dic[\"total_cost\"] = cb.total_cost\n",
        "\n",
        "        return answer_dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A_140_qa = ArticleQA(create_vector_db('markdown_articles/A_111a_2021_UsingFreshStarts.mmd'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s_r = sentence_similarity_gpt4(initial_prompt, \"Control, Holiday Framing, Birthday Framing\", h_answer)\n",
        "\n",
        "json.loads(s_r['choices'][0]['message']['function_call']['arguments'])['score']\n",
        "\n",
        "# 1 - mse(predicted, actual) / mse(mean of group, actual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def improve_prompt(\n",
        "    article_class, prompt, prompt_answer_format, desired_prompt_answer, history=None\n",
        "):\n",
        "    if history is None:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a prompt engineer that improves the accuracy of the prompt. You give the user a prompt and recieve a prompt performance score between 0 and 1 on the prompt you made. Then according to the score, you change the prompt to improve the score returning only the improved prompt.\",\n",
        "            },\n",
        "            {\"role\": \"assistant\", \"content\": prompt},\n",
        "        ]\n",
        "    else:\n",
        "        messages = history\n",
        "\n",
        "    q_and_a = article_class.query_context(prompt, prompt_answer_format)\n",
        "\n",
        "    q_and_a[\"gpt_answer\"][\"result\"]\n",
        "\n",
        "    print(\"document prompt:\", prompt)\n",
        "    print(\"prompt answer:\", q_and_a[\"gpt_answer\"][\"result\"])\n",
        "\n",
        "    sim_res = sentence_similarity_gpt4(\n",
        "        prompt, q_and_a[\"gpt_answer\"][\"result\"], desired_prompt_answer\n",
        "    )\n",
        "\n",
        "    similarity_score = json.loads(\n",
        "        sim_res[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"]\n",
        "    )[\"score\"]\n",
        "\n",
        "    print(\"prompt score:\", similarity_score)\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": str(similarity_score)})\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "\n",
        "    messages.append(\n",
        "        {\"role\": \"assistant\", \"content\": completion.choices[0].message.content}\n",
        "    )\n",
        "\n",
        "    return completion, messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "h_answer = \"delayed option takeup, immediate option takeup, immediate or delayed option takeup, average contribution rates (nov-june)\"\n",
        "\n",
        "initial_prompt = behavior_df.iloc[0]['Explanation']\n",
        "answer_format = behavior_df.iloc[0]['Coding scheme']\n",
        "\n",
        "histo = None\n",
        "\n",
        "\n",
        "while True:\n",
        "    \n",
        "    r, h = improve_prompt(A_140_qa, initial_prompt, answer_format, h_answer, histo)\n",
        "\n",
        "    print(\"new prompt:\", r.choices[0].message.content)\n",
        "    print(\"________________________________________________\")\n",
        "\n",
        "    initial_prompt = r.choices[0].message.content\n",
        "    histo = h\n",
        "\n",
        "    user_input = input(\"Do you want to continue (Y/N)? \").strip().lower()\n",
        "    if user_input != 'y':\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "h_answer = \"Control (C1 + C2), Holiday Placebo (C1), Holiday Placebo (C2), Fresh Start Framing (Birthday C1 and Holiday FS C1 + C2)\"\n",
        "\n",
        "initial_prompt = condition_df.iloc[0]['Explanation']\n",
        "answer_format = condition_df.iloc[0]['Coding scheme']\n",
        "\n",
        "histo = None\n",
        "\n",
        "\n",
        "while True:\n",
        "    \n",
        "    r, h = improve_prompt(A_140_qa, initial_prompt, answer_format, h_answer, histo)\n",
        "\n",
        "    print(\"new prompt:\", r.choices[0].message.content)\n",
        "    print(\"________________________________________________\")\n",
        "\n",
        "    initial_prompt = r.choices[0].message.content\n",
        "    histo = h\n",
        "\n",
        "    user_input = input(\"Do you want to continue (Y/N)? \").strip().lower()\n",
        "    if user_input != 'y':\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "r2, h2 = improve_prompt(A_140_qa, r.choices[0].message.content, condition_df.iloc[0]['Coding scheme'], h_answer, history=h)\n",
        "\n",
        "print(\"new prompt:\", r2.choices[0].message.content)\n",
        "print(\"history:\", h2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
